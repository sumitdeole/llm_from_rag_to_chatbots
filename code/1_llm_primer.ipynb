{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQEWtbLY3-xd"
      },
      "source": [
        "# Large Language Models\n",
        "\n",
        "This notebook is designed to seamlessly run both locally and on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0LbZbd4HZ-"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings 🛠️\n",
        "\n",
        "I'll install two libraries: Langchain and Llama.cpp. \n",
        "\n",
        "**LangChain** is a framework that simplifies the development of applications powered by large language models (LLMs)\n",
        "\n",
        "**llama.cpp** enables us to execute quantised versions of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_oTkGwppsp",
        "outputId": "f7bb55df-79de-4a3b-802d-53a0d4ca4b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "# As this notebook is originally on Colab, here we'll use their NVIDIA GPU and activate cuBLAS\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIolHmA0qIpT"
      },
      "source": [
        "Before we dive into the examples, let's download the large language model (LLM) we'll be using. For these exercises, we've selected a [quantised version of Mistral AI's Mistral 7B model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). \n",
        "\n",
        "> Quantisation of LLMs is a process that reduces the precision of the numerical values in the model, such as converting 32-bit floating-point numbers to 8-bit integers. These models are therefore smaller and faster, allowing them to run on less powerful hardware with only a small loss in precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKugu_x6qDbj",
        "outputId": "f5a56a52-1db9-4698-a9e9-bb4e2741c3cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmp8a1s5d58\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [00:39<00:00, 111MB/s] \n",
            "./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d2-gAb5Cw7"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up the LLM 🧠\n",
        "\n",
        "Langchain simplifies LLM deployment with its streamlined setup process. A single line of code configures the LLM, allowing to tailor the parameters to your specific needs.\n",
        "\n",
        "Here's a brief overview of some of the parameters:\n",
        "* **model_path:** The path to the Llama model file that will be used for generating text.\n",
        "* **max_tokens:** The maximum number of tokens that the model should generate in its response.\n",
        "* **temperature:** A value between 0 and 1 that controls the randomness of the model's generation. A lower temperature results in more predictable, constrained output, while a higher temperature yields more creative and diverse text.\n",
        "* **top_p:** A value between 0 and 1 that controls the diversity of the model's predictions. A higher top_p value prioritizes the most probable tokens, while a lower top_p value encourages the model to explore a wider range of possibilities.\n",
        "* **n_gpu_layers:** The default setting of 0 will cause all layers to be executed on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F76VzycqhLv",
        "outputId": "80ba035a-668d-4f07-b086-2191dad9b44c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.14 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =     1.14 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.12 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWG2XPn15NgV"
      },
      "source": [
        "---\n",
        "## 3.&nbsp; Asking LLM questions 🤖\n",
        "Let's play around and note how small changes make a big difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCFyGekD1YWQ",
        "outputId": "168512bc-aecb-4239-de70-fbe657eec97e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =     149.94 ms /   226 runs   (    0.66 ms per token,  1507.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     232.30 ms /     8 tokens (   29.04 ms per token,    34.44 tokens per second)\n",
            "llama_print_timings:        eval time =    5334.07 ms /   226 runs   (   23.60 ms per token,    42.37 tokens per second)\n",
            "llama_print_timings:       total time =    6673.09 ms /   234 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1. Polar Bears\n",
            "2. Arctic Foxes\n",
            "3. Walruses\n",
            "4. Caribou\n",
            "5. Beluga Whales\n",
            "6. Narwhals\n",
            "7. Seals\n",
            "8. Musk Oxen\n",
            "9. Arctic Hares\n",
            "10. Snowy Owls\n",
            "11. Reindeer\n",
            "12. Beavers\n",
            "13. Moose\n",
            "14. Lynx\n",
            "15. Wolverines\n",
            "16. Arctic Wolves\n",
            "17. Harp Seals\n",
            "18. Dall Sheep\n",
            "19. Pacific Walruses\n",
            "20. Harp Porpoises\n",
            "21. Pacific Salmon\n",
            "22. Arctic Char\n",
            "23. Dwarf Arctic Foxes\n",
            "24. Arctic Poppies\n",
            "25. Arctic Willows\n",
            "26. Arctic Cotton\n",
            "27. Arctic Cranberries\n",
            "28. Arctic Blueberries\n",
            "29. Arctic Crowberries\n",
            "30. Arctic Huckleberries\n"
          ]
        }
      ],
      "source": [
        "answer_1 = llm.invoke(\"Which animals live at the north pole?\")\n",
        "print(answer_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc8mzzu51d5b",
        "outputId": "cb182764-7990-49d3-a5b5-733b9d91ce5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =     166.65 ms /   299 runs   (    0.56 ms per token,  1794.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     110.36 ms /    12 tokens (    9.20 ms per token,   108.73 tokens per second)\n",
            "llama_print_timings:        eval time =    6941.35 ms /   298 runs   (   23.29 ms per token,    42.93 tokens per second)\n",
            "llama_print_timings:       total time =    8131.73 ms /   310 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "In the land of snow and ice,\n",
            "Where the sun doesn't shine nice,\n",
            "Lives a group of animals brave,\n",
            "Adapted to their cold environment.\n",
            "\n",
            "The polar bear is the king,\n",
            "Of this arctic realm so thick,\n",
            "With fur so white and thick,\n",
            "It keeps him warm from the cold trick.\n",
            "\n",
            "He hunts for seals to eat,\n",
            "On the frozen sea he meets,\n",
            "With his powerful paws and teeth,\n",
            "He can break through the thickest ice sheet.\n",
            "\n",
            "The arctic fox is next,\n",
            "With his coat of white and red,\n",
            "He blends in with his surroundings,\n",
            "And can run fast when he needs.\n",
            "\n",
            "He eats lemmings and birds,\n",
            "And keeps warm with his fur,\n",
            "His small size helps him conserve,\n",
            "Energy in this harsh world.\n",
            "\n",
            "The caribou roam free,\n",
            "With antlers tall and wide,\n",
            "They eat grasses and lichens,\n",
            "And can run fast when they need to hide.\n",
            "\n",
            "The arctic hare is small,\n",
            "But quick as a cheetah,\n",
            "He eats the leftovers,\n",
            "When the caribou have had enough.\n",
            "\n",
            "These animals live in harmony,\n",
            "In this land of snow and ice,\n",
            "They have adapted to their surroundings,\n",
            "And can survive in this cold paradise.\n"
          ]
        }
      ],
      "source": [
        "answer_2 = llm.invoke(\"Write a poem about animals that live at the north pole.\")\n",
        "print(answer_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3LAlLON1G3g",
        "outputId": "ed3174f7-2282-453f-ae96-40f4a3b0f7c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     232.35 ms\n",
            "llama_print_timings:      sample time =      86.72 ms /   136 runs   (    0.64 ms per token,  1568.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     134.51 ms /    15 tokens (    8.97 ms per token,   111.51 tokens per second)\n",
            "llama_print_timings:        eval time =    3147.49 ms /   135 runs   (   23.31 ms per token,    42.89 tokens per second)\n",
            "llama_print_timings:       total time =    3836.48 ms /   150 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The central limit theorem is a math rule that says when you take lots of numbers and add them up, it doesn't really matter where each number came from or how many there are - the result will still be pretty much normal. Like if you have a bunch of apples and bananas and you mix them together, the height of the pile won't be exactly average (because apples and bananas have different heights), but it will be pretty close to average. And if you have even more apples and bananas, the pile will get even closer to average. The more numbers you add, the closer the result gets to the middle.\n"
          ]
        }
      ],
      "source": [
        "answer_3 = llm.invoke(\"Explain the central limit theorem like I'm 5 years old.\")\n",
        "print(answer_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnvnmmRIYQhF"
      },
      "source": [
        "The answers provided by the 7B model may not seem as impressive as those from the latest OpenAI or Google models, but consider the significant size difference - they perform very well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
