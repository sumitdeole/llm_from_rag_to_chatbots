{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQEWtbLY3-xd"
      },
      "source": [
        "# Chatbot\n",
        "In this tutorial, we'll be designing a chatbot with the capability to retain information from previous prompts and responses, enabling it to maintain context throughout the conversation. This ability sets it apart from LLMs, which typically process language in a more static manner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0LbZbd4HZ-"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings üõ†Ô∏è\n",
        "Let's download and install the necessary libraries and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ_oTkGwppsp",
        "outputId": "3efc15b8-3cc1-4cc3-bb5b-94dbb2d0a03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmp6zeylnrv\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [00:20<00:00, 209MB/s]\n",
            "./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d2-gAb5Cw7"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up your LLM üß†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F76VzycqhLv",
        "outputId": "daaa3342-d449-40f7-e70e-a8ff87efc154"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.14 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =     1.14 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.12 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWG2XPn15NgV"
      },
      "source": [
        "### 2.1.&nbsp; Test your LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc8mzzu51d5b",
        "outputId": "aafbef54-1d04-4c82-a9e4-57c54a8ca6e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     269.28 ms\n",
            "llama_print_timings:      sample time =     110.52 ms /   183 runs   (    0.60 ms per token,  1655.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     269.22 ms /     8 tokens (   33.65 ms per token,    29.72 tokens per second)\n",
            "llama_print_timings:        eval time =    4441.99 ms /   182 runs   (   24.41 ms per token,    40.97 tokens per second)\n",
            "llama_print_timings:       total time =    5597.32 ms /   190 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data Science is the future, it's where we're all headed,\n",
            "With algorithms and models, predictions are guaranteed.\n",
            "We take in information from all around,\n",
            "And use it to create insights that astound.\n",
            "\n",
            "From machine learning to deep learning too,\n",
            "There's nothing that we can't do.\n",
            "We analyze data with techniques so precise,\n",
            "And make predictions with incredible accuracy.\n",
            "\n",
            "Data Science is the key to unlocking the unknown,\n",
            "It helps us make decisions that have never been shown.\n",
            "With statistical analysis and visualization tools,\n",
            "We can find patterns and make them known.\n",
            "\n",
            "So if you want to be ahead of the game,\n",
            "Learn Data Science, it's not a shame.\n",
            "With its endless possibilities and potential for growth,\n",
            "It's the future of technology, there's no doubt.\n"
          ]
        }
      ],
      "source": [
        "answer_1 = llm.invoke(\"Write a poem about data science.\")\n",
        "print(answer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhplYxpo4sb1"
      },
      "source": [
        "---\n",
        "## 3.&nbsp; Making a chatbot üí¨\n",
        "To transform a basic LLM into a chatbot, we'll need to infuse it with additional functionalities: prompts, memory, and chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iud1c8L4s2R"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "### Prompt ###\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"Keep your answers very short, succinct, and to the point.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "### Memory ###\n",
        "# We `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# `\"chat_history\"` must align with the MessagesPlaceholder name\n",
        "memory = ConversationBufferMemory(memory_key = \"chat_history\",\n",
        "                                  return_messages=True)\n",
        "\n",
        "### Chain ###\n",
        "conversation = LLMChain(llm = llm,\n",
        "                        prompt = prompt,\n",
        "                        verbose = True,\n",
        "                        memory = memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yym6NjddEKN"
      },
      "source": [
        "### 3.1.&nbsp; Test the chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umtqcoo-dC0b",
        "outputId": "5cf0d385-5fbd-48df-e9a6-71616f2edc89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Keep your answers very short, succinct, and to the point.\n",
            "Human: hi\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     269.28 ms\n",
            "llama_print_timings:      sample time =       8.79 ms /    13 runs   (    0.68 ms per token,  1478.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     216.19 ms /    21 tokens (   10.29 ms per token,    97.13 tokens per second)\n",
            "llama_print_timings:        eval time =     324.60 ms /    12 runs   (   27.05 ms per token,    36.97 tokens per second)\n",
            "llama_print_timings:       total time =     644.82 ms /    33 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?')],\n",
              " 'text': '\\nAI: Hello! How can I assist you today?'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsOUTUFdn0GO",
        "outputId": "9e0f11be-2965-497a-8791-89c25e8ec108"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Keep your answers very short, succinct, and to the point.\n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: Translate this sentence from English to French: I love programming.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     269.28 ms\n",
            "llama_print_timings:      sample time =       8.12 ms /    10 runs   (    0.81 ms per token,  1231.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     294.33 ms /    30 tokens (    9.81 ms per token,   101.93 tokens per second)\n",
            "llama_print_timings:        eval time =     218.00 ms /     9 runs   (   24.22 ms per token,    41.28 tokens per second)\n",
            "llama_print_timings:       total time =     585.81 ms /    39 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Translate this sentence from English to French: I love programming.',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='Translate this sentence from English to French: I love programming.'),\n",
              "  AIMessage(content='\\nAI: Je aime programmer.')],\n",
              " 'text': '\\nAI: Je aime programmer.'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"Translate this sentence from English to French: I love programming.\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RbaU6QJoAO1",
        "outputId": "0fefd45c-945f-40a7-d5ac-a28534589e44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Keep your answers very short, succinct, and to the point.\n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: Translate this sentence from English to French: I love programming.\n",
            "AI: \n",
            "AI: Je aime programmer.\n",
            "Human: Now translate the sentence to German.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     269.28 ms\n",
            "llama_print_timings:      sample time =       9.13 ms /    15 runs   (    0.61 ms per token,  1642.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     206.91 ms /    21 tokens (    9.85 ms per token,   101.49 tokens per second)\n",
            "llama_print_timings:        eval time =     378.42 ms /    14 runs   (   27.03 ms per token,    37.00 tokens per second)\n",
            "llama_print_timings:       total time =     666.37 ms /    35 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Now translate the sentence to German.',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='Translate this sentence from English to French: I love programming.'),\n",
              "  AIMessage(content='\\nAI: Je aime programmer.'),\n",
              "  HumanMessage(content='Now translate the sentence to German.'),\n",
              "  AIMessage(content='\\nAI: \\nAI: Ich liebe Programmieren.')],\n",
              " 'text': '\\nAI: \\nAI: Ich liebe Programmieren.'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"Now translate the sentence to German.\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5PGH3Y4ohYo",
        "outputId": "e9a56168-d915-445a-dd43-dfd4b5480596"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: Keep your answers very short, succinct, and to the point.\n",
            "Human: hi\n",
            "AI: \n",
            "AI: Hello! How can I assist you today?\n",
            "Human: Translate this sentence from English to French: I love programming.\n",
            "AI: \n",
            "AI: Je aime programmer.\n",
            "Human: Now translate the sentence to German.\n",
            "AI: \n",
            "AI: \n",
            "AI: Ich liebe Programmieren.\n",
            "Human: Which contains more characters, the French translation or the German?\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     269.28 ms\n",
            "llama_print_timings:      sample time =       7.68 ms /    15 runs   (    0.51 ms per token,  1952.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.79 ms /    27 tokens (   10.51 ms per token,    95.14 tokens per second)\n",
            "llama_print_timings:        eval time =     352.87 ms /    14 runs   (   25.20 ms per token,    39.67 tokens per second)\n",
            "llama_print_timings:       total time =     688.69 ms /    41 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Which contains more characters, the French translation or the German?',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='\\nAI: Hello! How can I assist you today?'),\n",
              "  HumanMessage(content='Translate this sentence from English to French: I love programming.'),\n",
              "  AIMessage(content='\\nAI: Je aime programmer.'),\n",
              "  HumanMessage(content='Now translate the sentence to German.'),\n",
              "  AIMessage(content='\\nAI: \\nAI: Ich liebe Programmieren.'),\n",
              "  HumanMessage(content='Which contains more characters, the French translation or the German?'),\n",
              "  AIMessage(content='\\nAI: \\nAI: The German translation contains more characters.')],\n",
              " 'text': '\\nAI: \\nAI: The German translation contains more characters.'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"Which contains more characters, the French translation or the German?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRsQ38ZS-3vU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
