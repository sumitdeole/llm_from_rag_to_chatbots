{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1n_By94muzv"
      },
      "source": [
        "## Deploy our chatbot as a webapp on Streamlit\n",
        "### 1.&nbsp; Install necessary libraries\n",
        "Let's install [streamlit](https://streamlit.io/) and [localtunnel](https://theboroer.github.io/localtunnel-www/), which allows us to run streamlit from colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HeA9pQPmuzx"
      },
      "outputs": [],
      "source": [
        "# Let's install streamlit\n",
        "!pip install -qqq streamlit --progress-bar off\n",
        "\n",
        "# Let's install localtunnel\n",
        "!npm install -qqq localtunnel --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL3JMjqgmuzy"
      },
      "source": [
        "We will also need to download the langchain and Mistral model as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGjvB9NXmuzy"
      },
      "outputs": [],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "!pip3 install -qqq sentence_transformers --progress-bar off\n",
        "!pip3 install -qqq faiss-gpu --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzQIB259muzy"
      },
      "source": [
        "### 2.&nbsp; Creating vector DB with embeddings\n",
        "We will build a RAG chain of the python version of [`An Introduction to Statistical Learning`](https://www.statlearning.com/) - a good reference for ML using Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bugExiphmuzy"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import LlamaCpp\n",
        "# llm\n",
        "\n",
        "!gdown 1NlZ_FOlM0IAB2vh5K_zrXdcBr0hoHyit\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/stats.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "llm = LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1,\n",
        "               n_ctx = 1024)\n",
        "\n",
        "# splitting\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                               chunk_overlap=200)\n",
        "\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# vector database\n",
        "vector_db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Let's save this database for later use\n",
        "vector_db.save_local(\"/content/faiss_index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPJCD8INmuzz"
      },
      "source": [
        "### 3.&nbsp; Let's deploy the model\n",
        "Now, let's proceed by creating the .py file for our rag chatbot.\n",
        "\n",
        "We sourced the foundational code for our Streamlit basic chatbot from the [Streamlit documentation](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o2Z13JOmuzz"
      },
      "outputs": [],
      "source": [
        "%%writefile streamlit_chatbot_app.py\n",
        "\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import streamlit as st\n",
        "\n",
        "# llm\n",
        "@st.cache_resource\n",
        "def init_llm():\n",
        "  return LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "                  max_tokens = 2000,\n",
        "                  temperature = 0.1,\n",
        "                  top_p = 1,\n",
        "                  n_gpu_layers = -1,\n",
        "                  n_ctx = 1024)\n",
        "llm = init_llm()\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/embeddings\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# load Vector Database produced earlier\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# memory\n",
        "@st.cache_resource\n",
        "def init_memory(_llm):\n",
        "    return ConversationBufferMemory(\n",
        "        llm=llm,\n",
        "        output_key='answer',\n",
        "        memory_key='chat_history',\n",
        "        return_messages=True)\n",
        "memory = init_memory(llm)\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are polite and professional question-answering AI assistant. You must provide a helpful response to the user.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
        "  (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
        "  (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer. If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
        "  (3) Ensure your answer answers the question, is helpful, professional, and formatted to be easily readable.\n",
        "[/INST]\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "[INST]\n",
        "Helpful Answer:\n",
        "[INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              retriever=retriever,\n",
        "                                              memory=memory,\n",
        "                                              return_source_documents=True,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})\n",
        "\n",
        "\n",
        "##### streamlit #####\n",
        "\n",
        "st.title(\"Ask me anything about Machine Learning using Python\")\n",
        "\n",
        "# Initialise chat history\n",
        "# Chat history saves the previous messages to be displayed\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"Curious minds wanted!\"):\n",
        "\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Begin spinner before answering question so it's there for the duration\n",
        "    with st.spinner(\"Going down the rabbithole for answers...\"):\n",
        "\n",
        "        # send question to chain to get answer\n",
        "        answer = chain(prompt)\n",
        "\n",
        "        # extract answer from dictionary returned by chain\n",
        "        response = answer[\"answer\"]\n",
        "\n",
        "        # Display chatbot response in chat message container\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(answer[\"answer\"])\n",
        "\n",
        "        # Add assistant response to chat history\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbOGFALqmuzz"
      },
      "outputs": [],
      "source": [
        "!streamlit run streamlit_chatbot_app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com\n",
        "# Use the IP address shown below as password to the webapp"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}