{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74fDzeoSRf2W"
      },
      "source": [
        "# Rag chatbot\n",
        "Let's now create a powerful tool: a RAG chain with memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voDa-iHRRnfc"
      },
      "source": [
        "---\n",
        "## 1.&nbsp; Installations and Settings üõ†Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8WEhS5-Ktmc",
        "outputId": "e3f40e94-d096-4849-c623-a81eec7f3ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmpzvgvot1y\n",
            "mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [00:44<00:00, 98.2MB/s]\n",
            "./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -qqq langchain --progress-bar off\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off\n",
        "!pip3 install -qqq sentence_transformers --progress-bar off\n",
        "!pip3 install -qqq faiss-gpu --progress-bar off\n",
        "\n",
        "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZAQCytqZqVl",
        "outputId": "1b89f9cf-9728-400f-e392-567a2f995522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "Successfully installed gdown-5.1.0\n",
            "Retrieving folder contents\n",
            "Processing file 1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt index.faiss\n",
            "Processing file 1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ index.pkl\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt\n",
            "To: /content/faiss_index/index.faiss\n",
            "100% 421k/421k [00:00<00:00, 2.96MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ\n",
            "To: /content/faiss_index/index.pkl\n",
            "100% 216k/216k [00:00<00:00, 2.14MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# in case you get the error 'NoneType' object has no attribute 'groups'\n",
        "!pip install --upgrade gdown\n",
        "\n",
        "# download saved vector database for Alice's Adventures in Wonderland\n",
        "!gdown --folder 1A8A9lhcUXUKRrtCe7rckMlQtgmfLZRQH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A61c2eNVTnyG"
      },
      "source": [
        "---\n",
        "## 2.&nbsp; Setting up the chain üîó\n",
        "There are 2 new items in this code that we haven't seen before:\n",
        "* the `output_key` parameter in [ConversationBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html)\n",
        "* [ConversationalRetrievalChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html#)\n",
        "\n",
        "The `ConversationalRetrievalChain` is the LangChain chain for RAG with memory.\n",
        "\n",
        "The `output_key` parameter is necessary if you want to include both `memory` and `return_source_documents` with `ConversationalRetrievalChain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3PlNxW1Jh5i",
        "outputId": "fe111b82-fdf4-410b-9223-f568a5981cfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 1024\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   128.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.16 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =     1.53 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.12 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# llm\n",
        "llm = LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "               max_tokens = 2000,\n",
        "               temperature = 0.1,\n",
        "               top_p = 1,\n",
        "               n_gpu_layers = -1,\n",
        "               n_ctx = 1024)\n",
        "\n",
        "# embeddings\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "embeddings_folder = \"/content/\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n",
        "                                   cache_folder=embeddings_folder)\n",
        "\n",
        "# load vector Database\n",
        "# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n",
        "vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# retriever\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# memory\n",
        "memory = ConversationBufferMemory(memory_key='chat_history',\n",
        "                                  return_messages=True,\n",
        "                                  output_key='answer')\n",
        "\n",
        "# prompt\n",
        "template = \"\"\"\n",
        "<s> [INST]\n",
        "You are polite and professional question-answering AI assistant. You must provide a helpful response to the user.\n",
        "\n",
        "In your response, PLEASE ALWAYS:\n",
        "  (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
        "  (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
        "  (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer. If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
        "  (3) Ensure your answer answers the question, is helpful, professional, and formatted to be easily readable.\n",
        "[/INST]\n",
        "[INST]\n",
        "Answer the following question using the context provided.\n",
        "The question is surrounded by the tags <q> </q>.\n",
        "The context is surrounded by the tags <c> </c>.\n",
        "<q>\n",
        "{question}\n",
        "</q>\n",
        "<c>\n",
        "{context}\n",
        "</c>\n",
        "[/INST]\n",
        "</s>\n",
        "[INST]\n",
        "Helpful Answer:\n",
        "[INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template,\n",
        "                        input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# chain\n",
        "chain = ConversationalRetrievalChain.from_llm(llm,\n",
        "                                              retriever=retriever,\n",
        "                                              memory=memory,\n",
        "                                              return_source_documents=True,\n",
        "                                              combine_docs_chain_kwargs={\"prompt\": prompt})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGWmLR5RO-Na",
        "outputId": "766148ed-b425-41f3-99d7-069e49c11fac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     248.74 ms\n",
            "llama_print_timings:      sample time =       8.68 ms /    13 runs   (    0.67 ms per token,  1497.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5283.32 ms /   520 tokens (   10.16 ms per token,    98.42 tokens per second)\n",
            "llama_print_timings:        eval time =     317.96 ms /    13 runs   (   24.46 ms per token,    40.89 tokens per second)\n",
            "llama_print_timings:       total time =    5828.58 ms /   533 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Who is the queen?',\n",
              " 'chat_history': [HumanMessage(content='Who is the queen?'),\n",
              "  AIMessage(content='The Queen in the story is the Queen of Hearts.')],\n",
              " 'answer': 'The Queen in the story is the Queen of Hearts.',\n",
              " 'source_documents': [Document(page_content='‚ÄúWould you tell me,‚Äù said Alice, a little timidly, ‚Äúwhy you are\\npainting those roses?‚Äù\\n\\nFive and Seven said nothing, but looked at Two. Two began in a low\\nvoice, ‚ÄúWhy the fact is, you see, Miss, this here ought to have been a\\n_red_ rose-tree, and we put a white one in by mistake; and if the Queen\\nwas to find it out, we should all have our heads cut off, you know. So\\nyou see, Miss, we‚Äôre doing our best, afore she comes, to‚Äî‚Äù At this\\nmoment Five, who had been anxiously looking across the garden, called\\nout ‚ÄúThe Queen! The Queen!‚Äù and the three gardeners instantly threw\\nthemselves flat upon their faces. There was a sound of many footsteps,\\nand Alice looked round, eager to see the Queen.', metadata={'source': '/Users/wbs/Documents/llm_project/alice_in_wonderland.txt'}),\n",
              "  Document(page_content='by without noticing her. Then followed the Knave of Hearts, carrying\\nthe King‚Äôs crown on a crimson velvet cushion; and, last of all this\\ngrand procession, came THE KING AND QUEEN OF HEARTS.', metadata={'source': '/Users/wbs/Documents/llm_project/alice_in_wonderland.txt'})]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\"Who is the queen?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuQsjOktO-Kg",
        "outputId": "1f29bc4a-bbc7-405f-fa5c-7f56441c69c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     248.74 ms\n",
            "llama_print_timings:      sample time =       5.65 ms /    11 runs   (    0.51 ms per token,  1947.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     716.32 ms /    75 tokens (    9.55 ms per token,   104.70 tokens per second)\n",
            "llama_print_timings:        eval time =     235.92 ms /    10 runs   (   23.59 ms per token,    42.39 tokens per second)\n",
            "llama_print_timings:       total time =    1019.78 ms /    85 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     248.74 ms\n",
            "llama_print_timings:      sample time =      48.40 ms /    88 runs   (    0.55 ms per token,  1818.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5382.31 ms /   548 tokens (    9.82 ms per token,   101.82 tokens per second)\n",
            "llama_print_timings:        eval time =    2132.17 ms /    87 runs   (   24.51 ms per token,    40.80 tokens per second)\n",
            "llama_print_timings:       total time =    7938.23 ms /   635 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but I couldn't find the answer to the question \"What does the Queen of Hearts enjoy doing?\" in the provided context. The provided context only includes information about the Queen of Hearts' actions during her trial, such as making tarts and stealing them from the Knave of Hearts. However, it does not provide any information about her hobbies or interests outside of her trial.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"What does she enjoy doing?\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLN-pj1UO-Fn",
        "outputId": "dfb12cd9-7492-407e-b7c6-b0ecc9cfc170"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     248.74 ms\n",
            "llama_print_timings:      sample time =       7.76 ms /    13 runs   (    0.60 ms per token,  1675.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1733.00 ms /   178 tokens (    9.74 ms per token,   102.71 tokens per second)\n",
            "llama_print_timings:        eval time =     282.99 ms /    12 runs   (   23.58 ms per token,    42.41 tokens per second)\n",
            "llama_print_timings:       total time =    2130.54 ms /   190 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     248.74 ms\n",
            "llama_print_timings:      sample time =      10.50 ms /    19 runs   (    0.55 ms per token,  1810.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5489.87 ms /   556 tokens (    9.87 ms per token,   101.28 tokens per second)\n",
            "llama_print_timings:        eval time =     445.89 ms /    18 runs   (   24.77 ms per token,    40.37 tokens per second)\n",
            "llama_print_timings:       total time =    6161.25 ms /   574 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Queen of Hearts chops off the head of the Knave of Hearts.\n"
          ]
        }
      ],
      "source": [
        "print(chain.invoke(\"Whose head does she chop off?\")[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFAIBPuHpOTn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
